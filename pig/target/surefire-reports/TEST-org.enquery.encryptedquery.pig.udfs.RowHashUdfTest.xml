<?xml version="1.0" encoding="UTF-8"?>
<testsuite xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="https://maven.apache.org/surefire/maven-surefire-plugin/xsd/surefire-test-report.xsd" name="org.enquery.encryptedquery.pig.udfs.RowHashUdfTest" time="6.303" tests="2" errors="2" skipped="0" failures="0">
  <properties>
    <property name="java.runtime.name" value="Java(TM) SE Runtime Environment"/>
    <property name="sun.boot.library.path" value="/opt/jdk1.8.0_171/jre/lib/amd64"/>
    <property name="java.vm.version" value="25.171-b11"/>
    <property name="java.vm.vendor" value="Oracle Corporation"/>
    <property name="maven.multiModuleProjectDirectory" value="/home/enquery/staging/EncryptedQuery/parent"/>
    <property name="javax.xml.accessExternalSchema" value="all"/>
    <property name="java.vendor.url" value="http://java.oracle.com/"/>
    <property name="path.separator" value=":"/>
    <property name="guice.disable.misplaced.annotation.check" value="true"/>
    <property name="java.vm.name" value="Java HotSpot(TM) 64-Bit Server VM"/>
    <property name="file.encoding.pkg" value="sun.io"/>
    <property name="user.country" value="US"/>
    <property name="sun.java.launcher" value="SUN_STANDARD"/>
    <property name="sun.os.patch.level" value="unknown"/>
    <property name="java.vm.specification.name" value="Java Virtual Machine Specification"/>
    <property name="user.dir" value="/home/enquery/staging/EncryptedQuery/parent"/>
    <property name="java.runtime.version" value="1.8.0_171-b11"/>
    <property name="java.awt.graphicsenv" value="sun.awt.X11GraphicsEnvironment"/>
    <property name="java.endorsed.dirs" value="/opt/jdk1.8.0_171/jre/lib/endorsed"/>
    <property name="os.arch" value="amd64"/>
    <property name="java.io.tmpdir" value="/tmp"/>
    <property name="line.separator" value="&#10;"/>
    <property name="java.vm.specification.vendor" value="Oracle Corporation"/>
    <property name="os.name" value="Linux"/>
    <property name="javax.xml.accessExternalDTD" value="all"/>
    <property name="classworlds.conf" value="/opt/apache-maven-3.6.0/bin/m2.conf"/>
    <property name="sun.jnu.encoding" value="UTF-8"/>
    <property name="java.library.path" value="/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib"/>
    <property name="maven.conf" value="/opt/apache-maven-3.6.0/conf"/>
    <property name="java.specification.name" value="Java Platform API Specification"/>
    <property name="java.class.version" value="52.0"/>
    <property name="sun.management.compiler" value="HotSpot 64-Bit Tiered Compilers"/>
    <property name="os.version" value="3.10.0-693.11.1.el7.x86_64"/>
    <property name="library.jansi.path" value="/opt/apache-maven-3.6.0/lib/jansi-native"/>
    <property name="user.home" value="/home/enquery"/>
    <property name="user.timezone" value="America/New_York"/>
    <property name="java.awt.printerjob" value="sun.print.PSPrinterJob"/>
    <property name="java.specification.version" value="1.8"/>
    <property name="file.encoding" value="UTF-8"/>
    <property name="user.name" value="enquery"/>
    <property name="java.class.path" value="/opt/apache-maven-3.6.0/boot/plexus-classworlds-2.5.2.jar"/>
    <property name="java.vm.specification.version" value="1.8"/>
    <property name="sun.arch.data.model" value="64"/>
    <property name="java.home" value="/opt/jdk1.8.0_171/jre"/>
    <property name="sun.java.command" value="org.codehaus.plexus.classworlds.launcher.Launcher clean install"/>
    <property name="java.specification.vendor" value="Oracle Corporation"/>
    <property name="user.language" value="en"/>
    <property name="awt.toolkit" value="sun.awt.X11.XToolkit"/>
    <property name="java.vm.info" value="mixed mode"/>
    <property name="java.version" value="1.8.0_171"/>
    <property name="java.ext.dirs" value="/opt/jdk1.8.0_171/jre/lib/ext:/usr/java/packages/lib/ext"/>
    <property name="securerandom.source" value="file:/dev/./urandom"/>
    <property name="sun.boot.class.path" value="/opt/jdk1.8.0_171/jre/lib/resources.jar:/opt/jdk1.8.0_171/jre/lib/rt.jar:/opt/jdk1.8.0_171/jre/lib/sunrsasign.jar:/opt/jdk1.8.0_171/jre/lib/jsse.jar:/opt/jdk1.8.0_171/jre/lib/jce.jar:/opt/jdk1.8.0_171/jre/lib/charsets.jar:/opt/jdk1.8.0_171/jre/lib/jfr.jar:/opt/jdk1.8.0_171/jre/classes"/>
    <property name="java.vendor" value="Oracle Corporation"/>
    <property name="maven.home" value="/opt/apache-maven-3.6.0"/>
    <property name="file.separator" value="/"/>
    <property name="java.vendor.url.bug" value="http://bugreport.sun.com/bugreport/"/>
    <property name="sun.cpu.endian" value="little"/>
    <property name="sun.io.unicode.encoding" value="UnicodeLittle"/>
    <property name="sun.cpu.isalist" value=""/>
  </properties>
  <testcase name="testSchema" classname="org.enquery.encryptedquery.pig.udfs.RowHashUdfTest" time="0.001">
    <error type="java.lang.ExceptionInInitializerError"><![CDATA[java.lang.ExceptionInInitializerError
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
Caused by: java.io.IOException: Too many open files
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
]]></error>
    <system-out><![CDATA[2018-11-21 14:56:07,756 [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: testClusterID
2018-11-21 14:56:07,876 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSEditLog  - Edit logging is async:true
2018-11-21 14:56:07,886 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - KeyProvider: null
2018-11-21 14:56:07,887 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - fsLock is fair: true
2018-11-21 14:56:07,887 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Detailed lock hold time metrics enabled: false
2018-11-21 14:56:07,906 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - fsOwner             = enquery (auth:SIMPLE)
2018-11-21 14:56:07,906 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - supergroup          = supergroup
2018-11-21 14:56:07,906 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - isPermissionEnabled = true
2018-11-21 14:56:07,906 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - HA Enabled: false
2018-11-21 14:56:07,936 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:07,940 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation  - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2018-11-21 14:56:07,940 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager  - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-11-21 14:56:07,940 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager  - dfs.namenode.datanode.registration.ip-hostname-check=true
2018-11-21 14:56:07,946 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-11-21 14:56:07,946 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - The block deletion will start around 2018 Nov 21 14:56:07
2018-11-21 14:56:07,947 [main] INFO  org.apache.hadoop.util.GSet  - Computing capacity for map BlocksMap
2018-11-21 14:56:07,947 [main] INFO  org.apache.hadoop.util.GSet  - VM type       = 64-bit
2018-11-21 14:56:07,949 [main] INFO  org.apache.hadoop.util.GSet  - 2.0% max memory 981.5 MB = 19.6 MB
2018-11-21 14:56:07,949 [main] INFO  org.apache.hadoop.util.GSet  - capacity      = 2^21 = 2097152 entries
2018-11-21 14:56:07,969 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - dfs.block.access.token.enable = false
2018-11-21 14:56:07,973 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation  - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2018-11-21 14:56:07,973 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode  - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-11-21 14:56:07,973 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode  - dfs.namenode.safemode.min.datanodes = 0
2018-11-21 14:56:07,973 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode  - dfs.namenode.safemode.extension = 0
2018-11-21 14:56:07,973 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - defaultReplication         = 3
2018-11-21 14:56:07,974 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - maxReplication             = 512
2018-11-21 14:56:07,974 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - minReplication             = 1
2018-11-21 14:56:07,974 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - maxReplicationStreams      = 2
2018-11-21 14:56:07,974 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - redundancyRecheckInterval  = 3000ms
2018-11-21 14:56:07,974 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - encryptDataTransfer        = false
2018-11-21 14:56:07,974 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - maxNumBlocksToLog          = 1000
2018-11-21 14:56:08,029 [main] INFO  org.apache.hadoop.util.GSet  - Computing capacity for map INodeMap
2018-11-21 14:56:08,029 [main] INFO  org.apache.hadoop.util.GSet  - VM type       = 64-bit
2018-11-21 14:56:08,029 [main] INFO  org.apache.hadoop.util.GSet  - 1.0% max memory 981.5 MB = 9.8 MB
2018-11-21 14:56:08,029 [main] INFO  org.apache.hadoop.util.GSet  - capacity      = 2^20 = 1048576 entries
2018-11-21 14:56:08,030 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSDirectory  - ACLs enabled? false
2018-11-21 14:56:08,030 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSDirectory  - POSIX ACL inheritance enabled? true
2018-11-21 14:56:08,030 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSDirectory  - XAttrs enabled? true
2018-11-21 14:56:08,031 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NameNode  - Caching file names occurring more than 10 times
2018-11-21 14:56:08,035 [main] INFO  org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager  - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true
2018-11-21 14:56:08,038 [main] INFO  org.apache.hadoop.util.GSet  - Computing capacity for map cachedBlocks
2018-11-21 14:56:08,039 [main] INFO  org.apache.hadoop.util.GSet  - VM type       = 64-bit
2018-11-21 14:56:08,039 [main] INFO  org.apache.hadoop.util.GSet  - 0.25% max memory 981.5 MB = 2.5 MB
2018-11-21 14:56:08,039 [main] INFO  org.apache.hadoop.util.GSet  - capacity      = 2^18 = 262144 entries
2018-11-21 14:56:08,046 [main] INFO  org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics  - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-11-21 14:56:08,046 [main] INFO  org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics  - NNTop conf: dfs.namenode.top.num.users = 10
2018-11-21 14:56:08,046 [main] INFO  org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics  - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-11-21 14:56:08,049 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Retry cache on namenode is enabled
2018-11-21 14:56:08,050 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-11-21 14:56:08,051 [main] INFO  org.apache.hadoop.util.GSet  - Computing capacity for map NameNodeRetryCache
2018-11-21 14:56:08,051 [main] INFO  org.apache.hadoop.util.GSet  - VM type       = 64-bit
2018-11-21 14:56:08,052 [main] INFO  org.apache.hadoop.util.GSet  - 0.029999999329447746% max memory 981.5 MB = 301.5 KB
2018-11-21 14:56:08,052 [main] INFO  org.apache.hadoop.util.GSet  - capacity      = 2^15 = 32768 entries
2018-11-21 14:56:08,076 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSImage  - Allocated new BlockPoolId: BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:08,141 [main] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1 has been successfully formatted.
2018-11-21 14:56:08,175 [main] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-2 has been successfully formatted.
2018-11-21 14:56:08,192 [FSImageSaver for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf  - Saving image file /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2018-11-21 14:56:08,192 [FSImageSaver for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf  - Saving image file /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2018-11-21 14:56:08,334 [FSImageSaver for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf  - Image file /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 392 bytes saved in 0 seconds.
2018-11-21 14:56:08,334 [FSImageSaver for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf  - Image file /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 392 bytes saved in 0 seconds.
2018-11-21 14:56:08,418 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager  - Going to retain 1 images with txid >= 0
2018-11-21 14:56:08,422 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NameNode  - createNameNode []
2018-11-21 14:56:08,542 [main] INFO  org.apache.commons.beanutils.FluentPropertyBeanIntrospector  - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2018-11-21 14:56:08,619 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
2018-11-21 14:56:08,665 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - Scheduled Metric snapshot period at 10 second(s).
2018-11-21 14:56:08,665 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - NameNode metrics system started
2018-11-21 14:56:08,687 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NameNode  - fs.defaultFS is hdfs://127.0.0.1:0
2018-11-21 14:56:08,715 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@79c7532f] INFO  org.apache.hadoop.util.JvmPauseMonitor  - Starting JVM pause monitor
2018-11-21 14:56:08,724 [main] INFO  org.apache.hadoop.hdfs.DFSUtil  - Starting Web-server for hdfs at: http://localhost:0
2018-11-21 14:56:08,735 [main] INFO  org.eclipse.jetty.util.log  - Logging initialized @1932ms
2018-11-21 14:56:08,802 [main] INFO  org.apache.hadoop.security.authentication.server.AuthenticationFilter  - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-11-21 14:56:08,803 [main] INFO  org.apache.hadoop.http.HttpRequestLog  - Http request log for http.requests.namenode is not defined
2018-11-21 14:56:08,809 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-11-21 14:56:08,810 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2018-11-21 14:56:08,810 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-11-21 14:56:08,810 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-11-21 14:56:08,827 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2018-11-21 14:56:08,828 [main] INFO  org.apache.hadoop.http.HttpServer2  - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2018-11-21 14:56:08,834 [main] INFO  org.apache.hadoop.http.HttpServer2  - Jetty bound to port 40344
2018-11-21 14:56:08,835 [main] INFO  org.eclipse.jetty.server.Server  - jetty-9.3.19.v20170502
2018-11-21 14:56:08,862 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@7bb6ab3a{/logs,file:///home/enquery/staging/EncryptedQuery/pig/target/test/logs,AVAILABLE}
2018-11-21 14:56:08,863 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@32c8e539{/static,jar:file:/home/enquery/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.0.1/hadoop-hdfs-3.0.1-tests.jar!/webapps/static,AVAILABLE}
2018-11-21 14:56:09,030 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.w.WebAppContext@388ba540{/,file:///tmp/jetty-localhost-40344-hdfs-_-any-4754174534249377938.dir/webapp/,AVAILABLE}{/hdfs}
2018-11-21 14:56:09,036 [main] INFO  org.eclipse.jetty.server.AbstractConnector  - Started ServerConnector@45cff11c{HTTP/1.1,[http/1.1]}{localhost:40344}
2018-11-21 14:56:09,036 [main] INFO  org.eclipse.jetty.server.Server  - Started @2232ms
2018-11-21 14:56:09,041 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSEditLog  - Edit logging is async:true
2018-11-21 14:56:09,041 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - KeyProvider: null
2018-11-21 14:56:09,041 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - fsLock is fair: true
2018-11-21 14:56:09,041 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Detailed lock hold time metrics enabled: false
2018-11-21 14:56:09,041 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - fsOwner             = enquery (auth:SIMPLE)
2018-11-21 14:56:09,041 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - supergroup          = supergroup
2018-11-21 14:56:09,041 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - isPermissionEnabled = true
2018-11-21 14:56:09,042 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - HA Enabled: false
2018-11-21 14:56:09,042 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:09,043 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager  - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-11-21 14:56:09,043 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager  - dfs.namenode.datanode.registration.ip-hostname-check=true
2018-11-21 14:56:09,043 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-11-21 14:56:09,043 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - The block deletion will start around 2018 Nov 21 14:56:09
2018-11-21 14:56:09,043 [main] INFO  org.apache.hadoop.util.GSet  - Computing capacity for map BlocksMap
2018-11-21 14:56:09,043 [main] INFO  org.apache.hadoop.util.GSet  - VM type       = 64-bit
2018-11-21 14:56:09,044 [main] INFO  org.apache.hadoop.util.GSet  - 2.0% max memory 910.5 MB = 18.2 MB
2018-11-21 14:56:09,044 [main] INFO  org.apache.hadoop.util.GSet  - capacity      = 2^21 = 2097152 entries
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - dfs.block.access.token.enable = false
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation  - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode  - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode  - dfs.namenode.safemode.min.datanodes = 0
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode  - dfs.namenode.safemode.extension = 0
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - defaultReplication         = 3
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - maxReplication             = 512
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - minReplication             = 1
2018-11-21 14:56:09,045 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - maxReplicationStreams      = 2
2018-11-21 14:56:09,046 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - redundancyRecheckInterval  = 3000ms
2018-11-21 14:56:09,046 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - encryptDataTransfer        = false
2018-11-21 14:56:09,046 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - maxNumBlocksToLog          = 1000
2018-11-21 14:56:09,046 [main] INFO  org.apache.hadoop.util.GSet  - Computing capacity for map INodeMap
2018-11-21 14:56:09,046 [main] INFO  org.apache.hadoop.util.GSet  - VM type       = 64-bit
2018-11-21 14:56:09,046 [main] INFO  org.apache.hadoop.util.GSet  - 1.0% max memory 910.5 MB = 9.1 MB
2018-11-21 14:56:09,046 [main] INFO  org.apache.hadoop.util.GSet  - capacity      = 2^20 = 1048576 entries
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSDirectory  - ACLs enabled? false
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSDirectory  - POSIX ACL inheritance enabled? true
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSDirectory  - XAttrs enabled? true
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NameNode  - Caching file names occurring more than 10 times
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager  - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.util.GSet  - Computing capacity for map cachedBlocks
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.util.GSet  - VM type       = 64-bit
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.util.GSet  - 0.25% max memory 910.5 MB = 2.3 MB
2018-11-21 14:56:09,047 [main] INFO  org.apache.hadoop.util.GSet  - capacity      = 2^18 = 262144 entries
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics  - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics  - NNTop conf: dfs.namenode.top.num.users = 10
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics  - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Retry cache on namenode is enabled
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.util.GSet  - Computing capacity for map NameNodeRetryCache
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.util.GSet  - VM type       = 64-bit
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.util.GSet  - 0.029999999329447746% max memory 910.5 MB = 279.7 KB
2018-11-21 14:56:09,048 [main] INFO  org.apache.hadoop.util.GSet  - capacity      = 2^15 = 32768 entries
2018-11-21 14:56:09,075 [main] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:09,100 [main] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:09,103 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FileJournalManager  - Recovering unfinalized segments in /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1/current
2018-11-21 14:56:09,103 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FileJournalManager  - Recovering unfinalized segments in /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-2/current
2018-11-21 14:56:09,103 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSImage  - No edit log streams selected.
2018-11-21 14:56:09,104 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSImage  - Planning to load image: FSImageFile(file=/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2018-11-21 14:56:09,145 [main] INFO  org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager  - Enable the erasure coding policy RS-6-3-1024k
2018-11-21 14:56:09,145 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode  - Loading 1 INodes.
2018-11-21 14:56:09,154 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf  - Loaded FSImage in 0 seconds.
2018-11-21 14:56:09,154 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSImage  - Loaded image for txid 0 from /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2018-11-21 14:56:09,159 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2018-11-21 14:56:09,160 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSEditLog  - Starting log segment at 1
2018-11-21 14:56:09,225 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NameCache  - initialized with 0 entries 0 lookups
2018-11-21 14:56:09,225 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Finished loading FSImage in 176 msecs
2018-11-21 14:56:09,396 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NameNode  - RPC server is binding to localhost:0
2018-11-21 14:56:09,404 [main] INFO  org.apache.hadoop.ipc.CallQueueManager  - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-11-21 14:56:09,413 [Socket Reader #1 for port 38997] INFO  org.apache.hadoop.ipc.Server  - Starting Socket Reader #1 for port 38997
2018-11-21 14:56:09,576 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NameNode  - Clients are to use localhost:38997 to access this namenode/service.
2018-11-21 14:56:09,579 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2018-11-21 14:56:09,593 [main] INFO  org.apache.hadoop.hdfs.server.namenode.LeaseManager  - Number of blocks under construction: 0
2018-11-21 14:56:09,608 [main] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - initializing replication queues
2018-11-21 14:56:09,608 [main] INFO  org.apache.hadoop.hdfs.StateChange  - STATE* Leaving safe mode after 0 secs
2018-11-21 14:56:09,608 [main] INFO  org.apache.hadoop.hdfs.StateChange  - STATE* Network topology has 0 racks and 0 datanodes
2018-11-21 14:56:09,608 [main] INFO  org.apache.hadoop.hdfs.StateChange  - STATE* UnderReplicatedBlocks has 0 blocks
2018-11-21 14:56:09,611 [Reconstruction Queue Initializer] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - Total number of blocks            = 0
2018-11-21 14:56:09,611 [Reconstruction Queue Initializer] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - Number of invalid blocks          = 0
2018-11-21 14:56:09,611 [Reconstruction Queue Initializer] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - Number of under-replicated blocks = 0
2018-11-21 14:56:09,611 [Reconstruction Queue Initializer] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - Number of  over-replicated blocks = 0
2018-11-21 14:56:09,611 [Reconstruction Queue Initializer] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockManager  - Number of blocks being written    = 0
2018-11-21 14:56:09,611 [Reconstruction Queue Initializer] INFO  org.apache.hadoop.hdfs.StateChange  - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 3 msec
2018-11-21 14:56:09,649 [IPC Server Responder] INFO  org.apache.hadoop.ipc.Server  - IPC Server Responder: starting
2018-11-21 14:56:09,650 [IPC Server listener on 38997] INFO  org.apache.hadoop.ipc.Server  - IPC Server listener on 38997: starting
2018-11-21 14:56:09,654 [main] INFO  org.apache.hadoop.hdfs.server.namenode.NameNode  - NameNode RPC up at: localhost/127.0.0.1:38997
2018-11-21 14:56:09,656 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem  - Starting services required for active state
2018-11-21 14:56:09,656 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSDirectory  - Initializing quota with 4 thread(s)
2018-11-21 14:56:09,659 [main] INFO  org.apache.hadoop.hdfs.server.namenode.FSDirectory  - Quota initialization completed in 3 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2018-11-21 14:56:09,663 [CacheReplicationMonitor(1592713292)] INFO  org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor  - Starting CacheReplicationMonitor with interval 30000 milliseconds
2018-11-21 14:56:09,671 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1,[DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2
2018-11-21 14:56:09,745 [main] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1
2018-11-21 14:56:09,755 [main] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2
2018-11-21 14:56:09,819 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - DataNode metrics system started (again)
2018-11-21 14:56:09,826 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:09,831 [main] INFO  org.apache.hadoop.hdfs.server.datanode.BlockScanner  - Initialized block scanner with targetBytesPerSec 1048576
2018-11-21 14:56:09,835 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Configured hostname is 127.0.0.1
2018-11-21 14:56:09,836 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:09,841 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Starting DataNode with maxLockedMemory = 0
2018-11-21 14:56:09,848 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Opened streaming server at /127.0.0.1:37623
2018-11-21 14:56:09,851 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Balancing bandwidth is 10485760 bytes/s
2018-11-21 14:56:09,851 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Number threads for balancing is 50
2018-11-21 14:56:09,860 [main] INFO  org.apache.hadoop.security.authentication.server.AuthenticationFilter  - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-11-21 14:56:09,860 [main] INFO  org.apache.hadoop.http.HttpRequestLog  - Http request log for http.requests.datanode is not defined
2018-11-21 14:56:09,862 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-11-21 14:56:09,863 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-11-21 14:56:09,863 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-11-21 14:56:09,863 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-11-21 14:56:09,867 [main] INFO  org.apache.hadoop.http.HttpServer2  - Jetty bound to port 34279
2018-11-21 14:56:09,867 [main] INFO  org.eclipse.jetty.server.Server  - jetty-9.3.19.v20170502
2018-11-21 14:56:09,869 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@4d4960c8{/logs,file:///home/enquery/staging/EncryptedQuery/pig/target/test/logs,AVAILABLE}
2018-11-21 14:56:09,869 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@773bd77b{/static,jar:file:/home/enquery/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.0.1/hadoop-hdfs-3.0.1-tests.jar!/webapps/static,AVAILABLE}
2018-11-21 14:56:10,003 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.w.WebAppContext@4bdcaf36{/,file:///tmp/jetty-localhost-34279-datanode-_-any-3266691875463392850.dir/webapp/,AVAILABLE}{/datanode}
2018-11-21 14:56:10,004 [main] INFO  org.eclipse.jetty.server.AbstractConnector  - Started ServerConnector@61d01788{HTTP/1.1,[http/1.1]}{localhost:34279}
2018-11-21 14:56:10,004 [main] INFO  org.eclipse.jetty.server.Server  - Started @3201ms
2018-11-21 14:56:10,193 [main] INFO  org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer  - Listening HTTP traffic on /127.0.0.1:43453
2018-11-21 14:56:10,194 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3a627c80] INFO  org.apache.hadoop.util.JvmPauseMonitor  - Starting JVM pause monitor
2018-11-21 14:56:10,195 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - dnUserName = enquery
2018-11-21 14:56:10,195 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - supergroup = supergroup
2018-11-21 14:56:10,207 [main] INFO  org.apache.hadoop.ipc.CallQueueManager  - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-11-21 14:56:10,207 [Socket Reader #1 for port 45964] INFO  org.apache.hadoop.ipc.Server  - Starting Socket Reader #1 for port 45964
2018-11-21 14:56:10,213 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Opened IPC server at /127.0.0.1:45964
2018-11-21 14:56:10,222 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Refresh request received for nameservices: null
2018-11-21 14:56:10,223 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Starting BPOfferServices for nameservices: <default>
2018-11-21 14:56:10,232 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38997 starting to offer service
2018-11-21 14:56:10,236 [IPC Server Responder] INFO  org.apache.hadoop.ipc.Server  - IPC Server Responder: starting
2018-11-21 14:56:10,236 [IPC Server listener on 45964] INFO  org.apache.hadoop.ipc.Server  - IPC Server listener on 45964: starting
2018-11-21 14:56:10,238 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3,[DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4
2018-11-21 14:56:10,240 [main] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3
2018-11-21 14:56:10,240 [main] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4
2018-11-21 14:56:10,300 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - DataNode metrics system started (again)
2018-11-21 14:56:10,301 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:10,301 [main] INFO  org.apache.hadoop.hdfs.server.datanode.BlockScanner  - Initialized block scanner with targetBytesPerSec 1048576
2018-11-21 14:56:10,301 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Configured hostname is 127.0.0.1
2018-11-21 14:56:10,302 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:10,302 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Starting DataNode with maxLockedMemory = 0
2018-11-21 14:56:10,302 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Opened streaming server at /127.0.0.1:43916
2018-11-21 14:56:10,303 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Balancing bandwidth is 10485760 bytes/s
2018-11-21 14:56:10,303 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Number threads for balancing is 50
2018-11-21 14:56:10,305 [main] INFO  org.apache.hadoop.security.authentication.server.AuthenticationFilter  - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-11-21 14:56:10,306 [main] INFO  org.apache.hadoop.http.HttpRequestLog  - Http request log for http.requests.datanode is not defined
2018-11-21 14:56:10,307 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-11-21 14:56:10,308 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-11-21 14:56:10,308 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-11-21 14:56:10,308 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-11-21 14:56:10,309 [main] INFO  org.apache.hadoop.http.HttpServer2  - Jetty bound to port 37355
2018-11-21 14:56:10,309 [main] INFO  org.eclipse.jetty.server.Server  - jetty-9.3.19.v20170502
2018-11-21 14:56:10,311 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@71870da7{/logs,file:///home/enquery/staging/EncryptedQuery/pig/target/test/logs,AVAILABLE}
2018-11-21 14:56:10,311 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@45792847{/static,jar:file:/home/enquery/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.0.1/hadoop-hdfs-3.0.1-tests.jar!/webapps/static,AVAILABLE}
2018-11-21 14:56:10,499 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38997
2018-11-21 14:56:10,503 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-11-21 14:56:10,525 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:10,526 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory with location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1 is not formatted for namespace 1665949480. Formatting...
2018-11-21 14:56:10,527 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Generated new storageID DS-7570c394-8de3-4d4c-afe8-3b112a6790c6 for directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1 
2018-11-21 14:56:10,545 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.w.WebAppContext@51dbd6e4{/,file:///tmp/jetty-localhost-37355-datanode-_-any-6631611002669301081.dir/webapp/,AVAILABLE}{/datanode}
2018-11-21 14:56:10,546 [main] INFO  org.eclipse.jetty.server.AbstractConnector  - Started ServerConnector@2b8bd14b{HTTP/1.1,[http/1.1]}{localhost:37355}
2018-11-21 14:56:10,546 [main] INFO  org.eclipse.jetty.server.Server  - Started @3743ms
2018-11-21 14:56:10,560 [main] INFO  org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer  - Listening HTTP traffic on /127.0.0.1:46170
2018-11-21 14:56:10,561 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5f303ecd] INFO  org.apache.hadoop.util.JvmPauseMonitor  - Starting JVM pause monitor
2018-11-21 14:56:10,561 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - dnUserName = enquery
2018-11-21 14:56:10,561 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - supergroup = supergroup
2018-11-21 14:56:10,562 [main] INFO  org.apache.hadoop.ipc.CallQueueManager  - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-11-21 14:56:10,562 [Socket Reader #1 for port 34589] INFO  org.apache.hadoop.ipc.Server  - Starting Socket Reader #1 for port 34589
2018-11-21 14:56:10,569 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Opened IPC server at /127.0.0.1:34589
2018-11-21 14:56:10,573 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Refresh request received for nameservices: null
2018-11-21 14:56:10,573 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Starting BPOfferServices for nameservices: <default>
2018-11-21 14:56:10,574 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38997 starting to offer service
2018-11-21 14:56:10,575 [IPC Server Responder] INFO  org.apache.hadoop.ipc.Server  - IPC Server Responder: starting
2018-11-21 14:56:10,575 [IPC Server listener on 34589] INFO  org.apache.hadoop.ipc.Server  - IPC Server listener on 34589: starting
2018-11-21 14:56:10,577 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38997
2018-11-21 14:56:10,577 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-11-21 14:56:10,578 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - Starting DataNode 2 with dfs.datanode.data.dir: [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5,[DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6
2018-11-21 14:56:10,579 [main] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5
2018-11-21 14:56:10,579 [main] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6
2018-11-21 14:56:10,600 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:10,601 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory with location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2 is not formatted for namespace 1665949480. Formatting...
2018-11-21 14:56:10,601 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Generated new storageID DS-a7d103ae-2c07-4e2d-9320-473283fa5c42 for directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2 
2018-11-21 14:56:10,626 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:10,626 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory with location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3 is not formatted for namespace 1665949480. Formatting...
2018-11-21 14:56:10,626 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Generated new storageID DS-97f0584c-068c-4bfc-875e-c3e0a305b65f for directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3 
2018-11-21 14:56:10,659 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - DataNode metrics system started (again)
2018-11-21 14:56:10,660 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:10,660 [main] INFO  org.apache.hadoop.hdfs.server.datanode.BlockScanner  - Initialized block scanner with targetBytesPerSec 1048576
2018-11-21 14:56:10,660 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Configured hostname is 127.0.0.1
2018-11-21 14:56:10,660 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:10,661 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Starting DataNode with maxLockedMemory = 0
2018-11-21 14:56:10,662 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Opened streaming server at /127.0.0.1:33372
2018-11-21 14:56:10,662 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Balancing bandwidth is 10485760 bytes/s
2018-11-21 14:56:10,662 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Number threads for balancing is 50
2018-11-21 14:56:10,666 [main] INFO  org.apache.hadoop.security.authentication.server.AuthenticationFilter  - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-11-21 14:56:10,666 [main] INFO  org.apache.hadoop.http.HttpRequestLog  - Http request log for http.requests.datanode is not defined
2018-11-21 14:56:10,668 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-11-21 14:56:10,669 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-11-21 14:56:10,669 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-11-21 14:56:10,669 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-11-21 14:56:10,670 [main] INFO  org.apache.hadoop.http.HttpServer2  - Jetty bound to port 39431
2018-11-21 14:56:10,670 [main] INFO  org.eclipse.jetty.server.Server  - jetty-9.3.19.v20170502
2018-11-21 14:56:10,672 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@7a344b65{/logs,file:///home/enquery/staging/EncryptedQuery/pig/target/test/logs,AVAILABLE}
2018-11-21 14:56:10,672 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@6b474074{/static,jar:file:/home/enquery/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.0.1/hadoop-hdfs-3.0.1-tests.jar!/webapps/static,AVAILABLE}
2018-11-21 14:56:10,734 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Analyzing storage directories for bpid BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:10,735 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Locking is disabled for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1/current/BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:10,735 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Block pool storage directory for location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1 and block pool id BP-117401205-192.168.200.57-1542830168067 is not formatted. Formatting ...
2018-11-21 14:56:10,736 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Formatting block pool BP-117401205-192.168.200.57-1542830168067 directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1/current/BP-117401205-192.168.200.57-1542830168067/current
2018-11-21 14:56:10,743 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:10,743 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory with location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4 is not formatted for namespace 1665949480. Formatting...
2018-11-21 14:56:10,743 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Generated new storageID DS-84b7d7ec-22f9-4191-aa8a-b2a19e766bb3 for directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4 
2018-11-21 14:56:10,826 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.w.WebAppContext@6be25526{/,file:///tmp/jetty-localhost-39431-datanode-_-any-2110556937874012270.dir/webapp/,AVAILABLE}{/datanode}
2018-11-21 14:56:10,826 [main] INFO  org.eclipse.jetty.server.AbstractConnector  - Started ServerConnector@42435b98{HTTP/1.1,[http/1.1]}{localhost:39431}
2018-11-21 14:56:10,826 [main] INFO  org.eclipse.jetty.server.Server  - Started @4023ms
2018-11-21 14:56:10,835 [main] INFO  org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer  - Listening HTTP traffic on /127.0.0.1:44456
2018-11-21 14:56:10,836 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - dnUserName = enquery
2018-11-21 14:56:10,836 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - supergroup = supergroup
2018-11-21 14:56:10,836 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@70e02081] INFO  org.apache.hadoop.util.JvmPauseMonitor  - Starting JVM pause monitor
2018-11-21 14:56:10,836 [main] INFO  org.apache.hadoop.ipc.CallQueueManager  - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-11-21 14:56:10,836 [Socket Reader #1 for port 35524] INFO  org.apache.hadoop.ipc.Server  - Starting Socket Reader #1 for port 35524
2018-11-21 14:56:10,838 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Opened IPC server at /127.0.0.1:35524
2018-11-21 14:56:10,841 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Refresh request received for nameservices: null
2018-11-21 14:56:10,841 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Starting BPOfferServices for nameservices: <default>
2018-11-21 14:56:10,841 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38997 starting to offer service
2018-11-21 14:56:10,842 [IPC Server Responder] INFO  org.apache.hadoop.ipc.Server  - IPC Server Responder: starting
2018-11-21 14:56:10,842 [IPC Server listener on 35524] INFO  org.apache.hadoop.ipc.Server  - IPC Server listener on 35524: starting
2018-11-21 14:56:10,843 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Analyzing storage directories for bpid BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:10,843 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38997
2018-11-21 14:56:10,843 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-11-21 14:56:10,844 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Locking is disabled for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2/current/BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:10,844 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Block pool storage directory for location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2 and block pool id BP-117401205-192.168.200.57-1542830168067 is not formatted. Formatting ...
2018-11-21 14:56:10,844 [Thread-67] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Formatting block pool BP-117401205-192.168.200.57-1542830168067 directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2/current/BP-117401205-192.168.200.57-1542830168067/current
2018-11-21 14:56:10,844 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - Starting DataNode 3 with dfs.datanode.data.dir: [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7,[DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8
2018-11-21 14:56:10,845 [main] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7
2018-11-21 14:56:10,845 [main] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8
2018-11-21 14:56:10,885 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Analyzing storage directories for bpid BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:10,885 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Locking is disabled for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3/current/BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:10,885 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Block pool storage directory for location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3 and block pool id BP-117401205-192.168.200.57-1542830168067 is not formatted. Formatting ...
2018-11-21 14:56:10,885 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Formatting block pool BP-117401205-192.168.200.57-1542830168067 directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3/current/BP-117401205-192.168.200.57-1542830168067/current
2018-11-21 14:56:10,893 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:10,894 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory with location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5 is not formatted for namespace 1665949480. Formatting...
2018-11-21 14:56:10,894 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Generated new storageID DS-4793ae8e-b081-4ea4-84b8-b1eff7e62a60 for directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5 
2018-11-21 14:56:10,918 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - DataNode metrics system started (again)
2018-11-21 14:56:10,919 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:10,919 [main] INFO  org.apache.hadoop.hdfs.server.datanode.BlockScanner  - Initialized block scanner with targetBytesPerSec 1048576
2018-11-21 14:56:10,919 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Configured hostname is 127.0.0.1
2018-11-21 14:56:10,919 [main] INFO  org.apache.hadoop.hdfs.server.common.Util  - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-11-21 14:56:10,919 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Starting DataNode with maxLockedMemory = 0
2018-11-21 14:56:10,920 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Opened streaming server at /127.0.0.1:37794
2018-11-21 14:56:10,920 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Balancing bandwidth is 10485760 bytes/s
2018-11-21 14:56:10,920 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Number threads for balancing is 50
2018-11-21 14:56:10,922 [main] INFO  org.apache.hadoop.security.authentication.server.AuthenticationFilter  - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-11-21 14:56:10,923 [main] INFO  org.apache.hadoop.http.HttpRequestLog  - Http request log for http.requests.datanode is not defined
2018-11-21 14:56:10,925 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-11-21 14:56:10,926 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-11-21 14:56:10,926 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-11-21 14:56:10,926 [main] INFO  org.apache.hadoop.http.HttpServer2  - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-11-21 14:56:10,926 [main] INFO  org.apache.hadoop.http.HttpServer2  - Jetty bound to port 34109
2018-11-21 14:56:10,926 [main] INFO  org.eclipse.jetty.server.Server  - jetty-9.3.19.v20170502
2018-11-21 14:56:10,929 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@2d746ce4{/logs,file:///home/enquery/staging/EncryptedQuery/pig/target/test/logs,AVAILABLE}
2018-11-21 14:56:10,929 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.s.ServletContextHandler@1948ea69{/static,jar:file:/home/enquery/.m2/repository/org/apache/hadoop/hadoop-hdfs/3.0.1/hadoop-hdfs-3.0.1-tests.jar!/webapps/static,AVAILABLE}
2018-11-21 14:56:10,973 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Setting up storage: nsid=1665949480;bpid=BP-117401205-192.168.200.57-1542830168067;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1665949480;c=1542830168067;bpid=BP-117401205-192.168.200.57-1542830168067;dnuuid=null
2018-11-21 14:56:10,994 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:10,994 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory with location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6 is not formatted for namespace 1665949480. Formatting...
2018-11-21 14:56:10,994 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Generated new storageID DS-a58e3cc8-2740-4452-a488-419318c8ec10 for directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6 
2018-11-21 14:56:11,061 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Analyzing storage directories for bpid BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,061 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Locking is disabled for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4/current/BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,061 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Block pool storage directory for location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4 and block pool id BP-117401205-192.168.200.57-1542830168067 is not formatted. Formatting ...
2018-11-21 14:56:11,061 [Thread-100] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Formatting block pool BP-117401205-192.168.200.57-1542830168067 directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4/current/BP-117401205-192.168.200.57-1542830168067/current
2018-11-21 14:56:11,087 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler  - Started o.e.j.w.WebAppContext@6c298dc{/,file:///tmp/jetty-localhost-34109-datanode-_-any-7409515344920954822.dir/webapp/,AVAILABLE}{/datanode}
2018-11-21 14:56:11,087 [main] INFO  org.eclipse.jetty.server.AbstractConnector  - Started ServerConnector@3e7dfd44{HTTP/1.1,[http/1.1]}{localhost:34109}
2018-11-21 14:56:11,087 [main] INFO  org.eclipse.jetty.server.Server  - Started @4284ms
2018-11-21 14:56:11,094 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Generated and persisted new Datanode UUID 75fafb3e-83e7-4cfd-a690-a36b15c89c1e
2018-11-21 14:56:11,103 [main] INFO  org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer  - Listening HTTP traffic on /127.0.0.1:37473
2018-11-21 14:56:11,103 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - dnUserName = enquery
2018-11-21 14:56:11,103 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - supergroup = supergroup
2018-11-21 14:56:11,103 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6b760460] INFO  org.apache.hadoop.util.JvmPauseMonitor  - Starting JVM pause monitor
2018-11-21 14:56:11,104 [main] INFO  org.apache.hadoop.ipc.CallQueueManager  - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-11-21 14:56:11,104 [Socket Reader #1 for port 39000] INFO  org.apache.hadoop.ipc.Server  - Starting Socket Reader #1 for port 39000
2018-11-21 14:56:11,107 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Opened IPC server at /127.0.0.1:39000
2018-11-21 14:56:11,109 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Refresh request received for nameservices: null
2018-11-21 14:56:11,109 [main] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Starting BPOfferServices for nameservices: <default>
2018-11-21 14:56:11,109 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38997 starting to offer service
2018-11-21 14:56:11,115 [IPC Server Responder] INFO  org.apache.hadoop.ipc.Server  - IPC Server Responder: starting
2018-11-21 14:56:11,116 [IPC Server listener on 39000] INFO  org.apache.hadoop.ipc.Server  - IPC Server listener on 39000: starting
2018-11-21 14:56:11,121 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38997
2018-11-21 14:56:11,123 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-11-21 14:56:11,127 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Analyzing storage directories for bpid BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,128 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Locking is disabled for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5/current/BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,128 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Block pool storage directory for location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5 and block pool id BP-117401205-192.168.200.57-1542830168067 is not formatted. Formatting ...
2018-11-21 14:56:11,128 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Formatting block pool BP-117401205-192.168.200.57-1542830168067 directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5/current/BP-117401205-192.168.200.57-1542830168067/current
2018-11-21 14:56:11,136 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Setting up storage: nsid=1665949480;bpid=BP-117401205-192.168.200.57-1542830168067;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1665949480;c=1542830168067;bpid=BP-117401205-192.168.200.57-1542830168067;dnuuid=null
2018-11-21 14:56:11,194 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Generated and persisted new Datanode UUID c8f50ee6-941e-455c-8eb6-01fa52eecdfa
2018-11-21 14:56:11,208 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added new volume: DS-7570c394-8de3-4d4c-afe8-3b112a6790c6
2018-11-21 14:56:11,208 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added volume - [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1, StorageType: DISK
2018-11-21 14:56:11,208 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added new volume: DS-97f0584c-068c-4bfc-875e-c3e0a305b65f
2018-11-21 14:56:11,209 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added volume - [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3, StorageType: DISK
2018-11-21 14:56:11,214 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added new volume: DS-a7d103ae-2c07-4e2d-9320-473283fa5c42
2018-11-21 14:56:11,214 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added volume - [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2, StorageType: DISK
2018-11-21 14:56:11,215 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added new volume: DS-84b7d7ec-22f9-4191-aa8a-b2a19e766bb3
2018-11-21 14:56:11,216 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added volume - [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4, StorageType: DISK
2018-11-21 14:56:11,219 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:11,219 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory with location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7 is not formatted for namespace 1665949480. Formatting...
2018-11-21 14:56:11,219 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Generated new storageID DS-dacc8259-5016-4843-86bb-08fffb5eedc1 for directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7 
2018-11-21 14:56:11,220 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Registered FSDatasetState MBean
2018-11-21 14:56:11,220 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Registered FSDatasetState MBean
2018-11-21 14:56:11,227 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1
2018-11-21 14:56:11,228 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3
2018-11-21 14:56:11,239 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker  - Scheduled health check for volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1
2018-11-21 14:56:11,239 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker  - Scheduled health check for volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3
2018-11-21 14:56:11,242 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2
2018-11-21 14:56:11,242 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4
2018-11-21 14:56:11,242 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker  - Scheduled health check for volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2
2018-11-21 14:56:11,242 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker  - Scheduled health check for volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4
2018-11-21 14:56:11,242 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,242 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,242 [Thread-185] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Scanning block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1...
2018-11-21 14:56:11,243 [Thread-187] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Scanning block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2...
2018-11-21 14:56:11,243 [Thread-186] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Scanning block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3...
2018-11-21 14:56:11,243 [Thread-188] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Scanning block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4...
2018-11-21 14:56:11,269 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Analyzing storage directories for bpid BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,269 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Locking is disabled for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6/current/BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,270 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Block pool storage directory for location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6 and block pool id BP-117401205-192.168.200.57-1542830168067 is not formatted. Formatting ...
2018-11-21 14:56:11,270 [Thread-135] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Formatting block pool BP-117401205-192.168.200.57-1542830168067 directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6/current/BP-117401205-192.168.200.57-1542830168067/current
2018-11-21 14:56:11,296 [Thread-186] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time taken to scan block pool BP-117401205-192.168.200.57-1542830168067 on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3: 52ms
2018-11-21 14:56:11,296 [Thread-187] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time taken to scan block pool BP-117401205-192.168.200.57-1542830168067 on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2: 53ms
2018-11-21 14:56:11,296 [Thread-185] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time taken to scan block pool BP-117401205-192.168.200.57-1542830168067 on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1: 53ms
2018-11-21 14:56:11,296 [Thread-188] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time taken to scan block pool BP-117401205-192.168.200.57-1542830168067 on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4: 52ms
2018-11-21 14:56:11,296 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Total time to scan all replicas for block pool BP-117401205-192.168.200.57-1542830168067: 54ms
2018-11-21 14:56:11,296 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Total time to scan all replicas for block pool BP-117401205-192.168.200.57-1542830168067: 54ms
2018-11-21 14:56:11,301 [Thread-193] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3...
2018-11-21 14:56:11,302 [Thread-195] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4...
2018-11-21 14:56:11,301 [Thread-194] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1...
2018-11-21 14:56:11,302 [Thread-196] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2...
2018-11-21 14:56:11,302 [Thread-196] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice  - Replica Cache file: /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2/current/BP-117401205-192.168.200.57-1542830168067/current/replicas doesn't exist 
2018-11-21 14:56:11,302 [Thread-193] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice  - Replica Cache file: /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3/current/BP-117401205-192.168.200.57-1542830168067/current/replicas doesn't exist 
2018-11-21 14:56:11,302 [Thread-194] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice  - Replica Cache file: /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1/current/BP-117401205-192.168.200.57-1542830168067/current/replicas doesn't exist 
2018-11-21 14:56:11,302 [Thread-195] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice  - Replica Cache file: /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4/current/BP-117401205-192.168.200.57-1542830168067/current/replicas doesn't exist 
2018-11-21 14:56:11,303 [Thread-194] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time to add replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1: 1ms
2018-11-21 14:56:11,303 [Thread-193] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time to add replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3: 2ms
2018-11-21 14:56:11,303 [Thread-196] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time to add replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2: 1ms
2018-11-21 14:56:11,303 [Thread-195] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time to add replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4: 1ms
2018-11-21 14:56:11,303 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Total time to add all replicas to map: 3ms
2018-11-21 14:56:11,303 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Total time to add all replicas to map: 3ms
2018-11-21 14:56:11,307 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - Now scanning bpid BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3
2018-11-21 14:56:11,307 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - Now scanning bpid BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4
2018-11-21 14:56:11,307 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - Now scanning bpid BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1
2018-11-21 14:56:11,307 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - Now scanning bpid BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2
2018-11-21 14:56:11,309 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3, DS-97f0584c-068c-4bfc-875e-c3e0a305b65f): finished scanning block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,309 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1, DS-7570c394-8de3-4d4c-afe8-3b112a6790c6): finished scanning block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,309 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4, DS-84b7d7ec-22f9-4191-aa8a-b2a19e766bb3): finished scanning block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,309 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2, DS-a7d103ae-2c07-4e2d-9320-473283fa5c42): finished scanning block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,319 [Thread-100] INFO  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - Periodic Directory Tree Verification scan starting at 11/21/18 4:19 PM with interval of 21600000ms
2018-11-21 14:56:11,319 [Thread-67] INFO  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - Periodic Directory Tree Verification scan starting at 11/21/18 5:16 PM with interval of 21600000ms
2018-11-21 14:56:11,327 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool BP-117401205-192.168.200.57-1542830168067 (Datanode Uuid 75fafb3e-83e7-4cfd-a690-a36b15c89c1e) service to localhost/127.0.0.1:38997 beginning handshake with NN
2018-11-21 14:56:11,327 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool BP-117401205-192.168.200.57-1542830168067 (Datanode Uuid c8f50ee6-941e-455c-8eb6-01fa52eecdfa) service to localhost/127.0.0.1:38997 beginning handshake with NN
2018-11-21 14:56:11,328 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Lock on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8/in_use.lock acquired by nodename 183575@pirk.envieta.com
2018-11-21 14:56:11,328 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Storage directory with location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8 is not formatted for namespace 1665949480. Formatting...
2018-11-21 14:56:11,328 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Generated new storageID DS-3db078d2-3115-422b-8225-a313bb44e226 for directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8 
2018-11-21 14:56:11,329 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data3, DS-97f0584c-068c-4bfc-875e-c3e0a305b65f): no suitable block pools found to scan.  Waiting 1814399978 ms.
2018-11-21 14:56:11,329 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data1, DS-7570c394-8de3-4d4c-afe8-3b112a6790c6): no suitable block pools found to scan.  Waiting 1814399978 ms.
2018-11-21 14:56:11,329 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data4, DS-84b7d7ec-22f9-4191-aa8a-b2a19e766bb3): no suitable block pools found to scan.  Waiting 1814399978 ms.
2018-11-21 14:56:11,329 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data2, DS-a7d103ae-2c07-4e2d-9320-473283fa5c42): no suitable block pools found to scan.  Waiting 1814399978 ms.
2018-11-21 14:56:11,340 [IPC Server handler 2 on 38997] INFO  org.apache.hadoop.hdfs.StateChange  - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43916, datanodeUuid=c8f50ee6-941e-455c-8eb6-01fa52eecdfa, infoPort=46170, infoSecurePort=0, ipcPort=34589, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067) storage c8f50ee6-941e-455c-8eb6-01fa52eecdfa
2018-11-21 14:56:11,341 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Setting up storage: nsid=1665949480;bpid=BP-117401205-192.168.200.57-1542830168067;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1665949480;c=1542830168067;bpid=BP-117401205-192.168.200.57-1542830168067;dnuuid=null
2018-11-21 14:56:11,343 [IPC Server handler 2 on 38997] INFO  org.apache.hadoop.net.NetworkTopology  - Adding a new node: /default-rack/127.0.0.1:43916
2018-11-21 14:56:11,344 [IPC Server handler 2 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager  - Registered DN c8f50ee6-941e-455c-8eb6-01fa52eecdfa (127.0.0.1:43916).
2018-11-21 14:56:11,347 [IPC Server handler 3 on 38997] INFO  org.apache.hadoop.hdfs.StateChange  - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:37623, datanodeUuid=75fafb3e-83e7-4cfd-a690-a36b15c89c1e, infoPort=43453, infoSecurePort=0, ipcPort=45964, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067) storage 75fafb3e-83e7-4cfd-a690-a36b15c89c1e
2018-11-21 14:56:11,347 [IPC Server handler 3 on 38997] INFO  org.apache.hadoop.net.NetworkTopology  - Adding a new node: /default-rack/127.0.0.1:37623
2018-11-21 14:56:11,347 [IPC Server handler 3 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager  - Registered DN 75fafb3e-83e7-4cfd-a690-a36b15c89c1e (127.0.0.1:37623).
2018-11-21 14:56:11,351 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool Block pool BP-117401205-192.168.200.57-1542830168067 (Datanode Uuid 75fafb3e-83e7-4cfd-a690-a36b15c89c1e) service to localhost/127.0.0.1:38997 successfully registered with NN
2018-11-21 14:56:11,351 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool Block pool BP-117401205-192.168.200.57-1542830168067 (Datanode Uuid c8f50ee6-941e-455c-8eb6-01fa52eecdfa) service to localhost/127.0.0.1:38997 successfully registered with NN
2018-11-21 14:56:11,351 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - For namenode localhost/127.0.0.1:38997 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-11-21 14:56:11,351 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - For namenode localhost/127.0.0.1:38997 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-11-21 14:56:11,383 [IPC Server handler 5 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor  - Adding new storage ID DS-7570c394-8de3-4d4c-afe8-3b112a6790c6 for DN 127.0.0.1:37623
2018-11-21 14:56:11,384 [IPC Server handler 5 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor  - Adding new storage ID DS-a7d103ae-2c07-4e2d-9320-473283fa5c42 for DN 127.0.0.1:37623
2018-11-21 14:56:11,385 [IPC Server handler 4 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor  - Adding new storage ID DS-97f0584c-068c-4bfc-875e-c3e0a305b65f for DN 127.0.0.1:43916
2018-11-21 14:56:11,386 [IPC Server handler 4 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor  - Adding new storage ID DS-84b7d7ec-22f9-4191-aa8a-b2a19e766bb3 for DN 127.0.0.1:43916
2018-11-21 14:56:11,395 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Generated and persisted new Datanode UUID b4135ae8-92a5-4afe-9f12-c00eaa92b614
2018-11-21 14:56:11,397 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added new volume: DS-4793ae8e-b081-4ea4-84b8-b1eff7e62a60
2018-11-21 14:56:11,397 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added volume - [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5, StorageType: DISK
2018-11-21 14:56:11,399 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added new volume: DS-a58e3cc8-2740-4452-a488-419318c8ec10
2018-11-21 14:56:11,399 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added volume - [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6, StorageType: DISK
2018-11-21 14:56:11,400 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Registered FSDatasetState MBean
2018-11-21 14:56:11,410 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5
2018-11-21 14:56:11,411 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker  - Scheduled health check for volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5
2018-11-21 14:56:11,411 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6
2018-11-21 14:56:11,411 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker  - Scheduled health check for volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6
2018-11-21 14:56:11,411 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,412 [Thread-206] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Scanning block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5...
2018-11-21 14:56:11,412 [Thread-207] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Scanning block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6...
2018-11-21 14:56:11,437 [Thread-207] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time taken to scan block pool BP-117401205-192.168.200.57-1542830168067 on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6: 25ms
2018-11-21 14:56:11,437 [Thread-206] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time taken to scan block pool BP-117401205-192.168.200.57-1542830168067 on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5: 25ms
2018-11-21 14:56:11,437 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Total time to scan all replicas for block pool BP-117401205-192.168.200.57-1542830168067: 26ms
2018-11-21 14:56:11,438 [Thread-210] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5...
2018-11-21 14:56:11,438 [Thread-211] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6...
2018-11-21 14:56:11,438 [Thread-210] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice  - Replica Cache file: /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5/current/BP-117401205-192.168.200.57-1542830168067/current/replicas doesn't exist 
2018-11-21 14:56:11,439 [Thread-211] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice  - Replica Cache file: /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6/current/BP-117401205-192.168.200.57-1542830168067/current/replicas doesn't exist 
2018-11-21 14:56:11,439 [Thread-210] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time to add replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5: 1ms
2018-11-21 14:56:11,439 [Thread-211] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time to add replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6: 0ms
2018-11-21 14:56:11,439 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Total time to add all replicas to map: 2ms
2018-11-21 14:56:11,439 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - Now scanning bpid BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5
2018-11-21 14:56:11,439 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5, DS-4793ae8e-b081-4ea4-84b8-b1eff7e62a60): finished scanning block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,439 [Thread-135] INFO  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - Periodic Directory Tree Verification scan starting at 11/21/18 4:31 PM with interval of 21600000ms
2018-11-21 14:56:11,440 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - Now scanning bpid BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6
2018-11-21 14:56:11,440 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool BP-117401205-192.168.200.57-1542830168067 (Datanode Uuid b4135ae8-92a5-4afe-9f12-c00eaa92b614) service to localhost/127.0.0.1:38997 beginning handshake with NN
2018-11-21 14:56:11,440 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data5, DS-4793ae8e-b081-4ea4-84b8-b1eff7e62a60): no suitable block pools found to scan.  Waiting 1814399999 ms.
2018-11-21 14:56:11,440 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x1ae418b3fdb2cd4c: Processing first storage report for DS-97f0584c-068c-4bfc-875e-c3e0a305b65f from datanode c8f50ee6-941e-455c-8eb6-01fa52eecdfa
2018-11-21 14:56:11,440 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6, DS-a58e3cc8-2740-4452-a488-419318c8ec10): finished scanning block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,441 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data6, DS-a58e3cc8-2740-4452-a488-419318c8ec10): no suitable block pools found to scan.  Waiting 1814399998 ms.
2018-11-21 14:56:11,443 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x1ae418b3fdb2cd4c: from storage DS-97f0584c-068c-4bfc-875e-c3e0a305b65f node DatanodeRegistration(127.0.0.1:43916, datanodeUuid=c8f50ee6-941e-455c-8eb6-01fa52eecdfa, infoPort=46170, infoSecurePort=0, ipcPort=34589, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067), blocks: 0, hasStaleStorage: true, processing time: 3 msecs, invalidatedBlocks: 0
2018-11-21 14:56:11,443 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x44964245823481a5: Processing first storage report for DS-a7d103ae-2c07-4e2d-9320-473283fa5c42 from datanode 75fafb3e-83e7-4cfd-a690-a36b15c89c1e
2018-11-21 14:56:11,443 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x44964245823481a5: from storage DS-a7d103ae-2c07-4e2d-9320-473283fa5c42 node DatanodeRegistration(127.0.0.1:37623, datanodeUuid=75fafb3e-83e7-4cfd-a690-a36b15c89c1e, infoPort=43453, infoSecurePort=0, ipcPort=45964, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2018-11-21 14:56:11,443 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x1ae418b3fdb2cd4c: Processing first storage report for DS-84b7d7ec-22f9-4191-aa8a-b2a19e766bb3 from datanode c8f50ee6-941e-455c-8eb6-01fa52eecdfa
2018-11-21 14:56:11,443 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x1ae418b3fdb2cd4c: from storage DS-84b7d7ec-22f9-4191-aa8a-b2a19e766bb3 node DatanodeRegistration(127.0.0.1:43916, datanodeUuid=c8f50ee6-941e-455c-8eb6-01fa52eecdfa, infoPort=46170, infoSecurePort=0, ipcPort=34589, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2018-11-21 14:56:11,444 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x44964245823481a5: Processing first storage report for DS-7570c394-8de3-4d4c-afe8-3b112a6790c6 from datanode 75fafb3e-83e7-4cfd-a690-a36b15c89c1e
2018-11-21 14:56:11,444 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x44964245823481a5: from storage DS-7570c394-8de3-4d4c-afe8-3b112a6790c6 node DatanodeRegistration(127.0.0.1:37623, datanodeUuid=75fafb3e-83e7-4cfd-a690-a36b15c89c1e, infoPort=43453, infoSecurePort=0, ipcPort=45964, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2018-11-21 14:56:11,444 [IPC Server handler 7 on 38997] INFO  org.apache.hadoop.hdfs.StateChange  - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:33372, datanodeUuid=b4135ae8-92a5-4afe-9f12-c00eaa92b614, infoPort=44456, infoSecurePort=0, ipcPort=35524, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067) storage b4135ae8-92a5-4afe-9f12-c00eaa92b614
2018-11-21 14:56:11,444 [IPC Server handler 7 on 38997] INFO  org.apache.hadoop.net.NetworkTopology  - Adding a new node: /default-rack/127.0.0.1:33372
2018-11-21 14:56:11,444 [IPC Server handler 7 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager  - Registered DN b4135ae8-92a5-4afe-9f12-c00eaa92b614 (127.0.0.1:33372).
2018-11-21 14:56:11,445 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Analyzing storage directories for bpid BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,445 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool Block pool BP-117401205-192.168.200.57-1542830168067 (Datanode Uuid b4135ae8-92a5-4afe-9f12-c00eaa92b614) service to localhost/127.0.0.1:38997 successfully registered with NN
2018-11-21 14:56:11,445 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - For namenode localhost/127.0.0.1:38997 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-11-21 14:56:11,445 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Locking is disabled for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7/current/BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,445 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Block pool storage directory for location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7 and block pool id BP-117401205-192.168.200.57-1542830168067 is not formatted. Formatting ...
2018-11-21 14:56:11,445 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Formatting block pool BP-117401205-192.168.200.57-1542830168067 directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7/current/BP-117401205-192.168.200.57-1542830168067/current
2018-11-21 14:56:11,448 [IPC Server handler 9 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor  - Adding new storage ID DS-4793ae8e-b081-4ea4-84b8-b1eff7e62a60 for DN 127.0.0.1:33372
2018-11-21 14:56:11,448 [IPC Server handler 9 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor  - Adding new storage ID DS-a58e3cc8-2740-4452-a488-419318c8ec10 for DN 127.0.0.1:33372
2018-11-21 14:56:11,450 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0xb4fad1093b732654: Processing first storage report for DS-a58e3cc8-2740-4452-a488-419318c8ec10 from datanode b4135ae8-92a5-4afe-9f12-c00eaa92b614
2018-11-21 14:56:11,450 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0xb4fad1093b732654: from storage DS-a58e3cc8-2740-4452-a488-419318c8ec10 node DatanodeRegistration(127.0.0.1:33372, datanodeUuid=b4135ae8-92a5-4afe-9f12-c00eaa92b614, infoPort=44456, infoSecurePort=0, ipcPort=35524, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2018-11-21 14:56:11,450 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0xb4fad1093b732654: Processing first storage report for DS-4793ae8e-b081-4ea4-84b8-b1eff7e62a60 from datanode b4135ae8-92a5-4afe-9f12-c00eaa92b614
2018-11-21 14:56:11,450 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0xb4fad1093b732654: from storage DS-4793ae8e-b081-4ea4-84b8-b1eff7e62a60 node DatanodeRegistration(127.0.0.1:33372, datanodeUuid=b4135ae8-92a5-4afe-9f12-c00eaa92b614, infoPort=44456, infoSecurePort=0, ipcPort=35524, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2018-11-21 14:56:11,475 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Successfully sent block report 0x44964245823481a5,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 7 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-21 14:56:11,475 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Successfully sent block report 0xb4fad1093b732654,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 1 msec to generate and 25 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-21 14:56:11,475 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Successfully sent block report 0x1ae418b3fdb2cd4c,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 7 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-21 14:56:11,475 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Got finalize command for block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,475 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Got finalize command for block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,475 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Got finalize command for block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,535 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - dnInfo.length != numDataNodes
2018-11-21 14:56:11,535 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - Waiting for cluster to become active
2018-11-21 14:56:11,545 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Analyzing storage directories for bpid BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,545 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Locking is disabled for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8/current/BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,545 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Block pool storage directory for location [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8 and block pool id BP-117401205-192.168.200.57-1542830168067 is not formatted. Formatting ...
2018-11-21 14:56:11,545 [Thread-169] INFO  org.apache.hadoop.hdfs.server.common.Storage  - Formatting block pool BP-117401205-192.168.200.57-1542830168067 directory /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8/current/BP-117401205-192.168.200.57-1542830168067/current
2018-11-21 14:56:11,612 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Setting up storage: nsid=1665949480;bpid=BP-117401205-192.168.200.57-1542830168067;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1665949480;c=1542830168067;bpid=BP-117401205-192.168.200.57-1542830168067;dnuuid=null
2018-11-21 14:56:11,638 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - dnInfo.length != numDataNodes
2018-11-21 14:56:11,638 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - Waiting for cluster to become active
2018-11-21 14:56:11,662 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Generated and persisted new Datanode UUID 61b799ec-4cbf-4a0c-8b8d-445c84972a85
2018-11-21 14:56:11,664 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added new volume: DS-dacc8259-5016-4843-86bb-08fffb5eedc1
2018-11-21 14:56:11,664 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added volume - [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7, StorageType: DISK
2018-11-21 14:56:11,665 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added new volume: DS-3db078d2-3115-422b-8225-a313bb44e226
2018-11-21 14:56:11,666 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Added volume - [DISK]file:/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8, StorageType: DISK
2018-11-21 14:56:11,666 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Registered FSDatasetState MBean
2018-11-21 14:56:11,667 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7
2018-11-21 14:56:11,668 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker  - Scheduled health check for volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7
2018-11-21 14:56:11,668 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker  - Scheduling a check for /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8
2018-11-21 14:56:11,668 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker  - Scheduled health check for volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8
2018-11-21 14:56:11,668 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,669 [Thread-221] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Scanning block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7...
2018-11-21 14:56:11,669 [Thread-222] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Scanning block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8...
2018-11-21 14:56:11,690 [Thread-222] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time taken to scan block pool BP-117401205-192.168.200.57-1542830168067 on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8: 21ms
2018-11-21 14:56:11,690 [Thread-221] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time taken to scan block pool BP-117401205-192.168.200.57-1542830168067 on /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7: 21ms
2018-11-21 14:56:11,691 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Total time to scan all replicas for block pool BP-117401205-192.168.200.57-1542830168067: 22ms
2018-11-21 14:56:11,691 [Thread-225] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7...
2018-11-21 14:56:11,691 [Thread-226] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Adding replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8...
2018-11-21 14:56:11,691 [Thread-225] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice  - Replica Cache file: /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7/current/BP-117401205-192.168.200.57-1542830168067/current/replicas doesn't exist 
2018-11-21 14:56:11,692 [Thread-226] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice  - Replica Cache file: /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8/current/BP-117401205-192.168.200.57-1542830168067/current/replicas doesn't exist 
2018-11-21 14:56:11,692 [Thread-225] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time to add replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7: 1ms
2018-11-21 14:56:11,692 [Thread-226] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Time to add replicas to map for block pool BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8: 1ms
2018-11-21 14:56:11,692 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl  - Total time to add all replicas to map: 2ms
2018-11-21 14:56:11,692 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - Now scanning bpid BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8
2018-11-21 14:56:11,692 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - Now scanning bpid BP-117401205-192.168.200.57-1542830168067 on volume /home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7
2018-11-21 14:56:11,692 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8, DS-3db078d2-3115-422b-8225-a313bb44e226): finished scanning block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,692 [Thread-169] INFO  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - Periodic Directory Tree Verification scan starting at 11/21/18 8:28 PM with interval of 21600000ms
2018-11-21 14:56:11,692 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7, DS-dacc8259-5016-4843-86bb-08fffb5eedc1): finished scanning block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,693 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool BP-117401205-192.168.200.57-1542830168067 (Datanode Uuid 61b799ec-4cbf-4a0c-8b8d-445c84972a85) service to localhost/127.0.0.1:38997 beginning handshake with NN
2018-11-21 14:56:11,693 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data8, DS-3db078d2-3115-422b-8225-a313bb44e226): no suitable block pools found to scan.  Waiting 1814399999 ms.
2018-11-21 14:56:11,693 [VolumeScannerThread(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7)] INFO  org.apache.hadoop.hdfs.server.datanode.VolumeScanner  - VolumeScanner(/home/enquery/staging/EncryptedQuery/pig/target/test/data/dfs/data/data7, DS-dacc8259-5016-4843-86bb-08fffb5eedc1): no suitable block pools found to scan.  Waiting 1814399999 ms.
2018-11-21 14:56:11,695 [IPC Server handler 2 on 38997] INFO  org.apache.hadoop.hdfs.StateChange  - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:37794, datanodeUuid=61b799ec-4cbf-4a0c-8b8d-445c84972a85, infoPort=37473, infoSecurePort=0, ipcPort=39000, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067) storage 61b799ec-4cbf-4a0c-8b8d-445c84972a85
2018-11-21 14:56:11,695 [IPC Server handler 2 on 38997] INFO  org.apache.hadoop.net.NetworkTopology  - Adding a new node: /default-rack/127.0.0.1:37794
2018-11-21 14:56:11,695 [IPC Server handler 2 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager  - Registered DN 61b799ec-4cbf-4a0c-8b8d-445c84972a85 (127.0.0.1:37794).
2018-11-21 14:56:11,696 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Block pool Block pool BP-117401205-192.168.200.57-1542830168067 (Datanode Uuid 61b799ec-4cbf-4a0c-8b8d-445c84972a85) service to localhost/127.0.0.1:38997 successfully registered with NN
2018-11-21 14:56:11,696 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - For namenode localhost/127.0.0.1:38997 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-11-21 14:56:11,699 [IPC Server handler 5 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor  - Adding new storage ID DS-dacc8259-5016-4843-86bb-08fffb5eedc1 for DN 127.0.0.1:37794
2018-11-21 14:56:11,699 [IPC Server handler 5 on 38997] INFO  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor  - Adding new storage ID DS-3db078d2-3115-422b-8225-a313bb44e226 for DN 127.0.0.1:37794
2018-11-21 14:56:11,701 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x794ad06166323063: Processing first storage report for DS-dacc8259-5016-4843-86bb-08fffb5eedc1 from datanode 61b799ec-4cbf-4a0c-8b8d-445c84972a85
2018-11-21 14:56:11,701 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x794ad06166323063: from storage DS-dacc8259-5016-4843-86bb-08fffb5eedc1 node DatanodeRegistration(127.0.0.1:37794, datanodeUuid=61b799ec-4cbf-4a0c-8b8d-445c84972a85, infoPort=37473, infoSecurePort=0, ipcPort=39000, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2018-11-21 14:56:11,702 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x794ad06166323063: Processing first storage report for DS-3db078d2-3115-422b-8225-a313bb44e226 from datanode 61b799ec-4cbf-4a0c-8b8d-445c84972a85
2018-11-21 14:56:11,702 [Block report processor] INFO  BlockStateChange  - BLOCK* processReport 0x794ad06166323063: from storage DS-3db078d2-3115-422b-8225-a313bb44e226 node DatanodeRegistration(127.0.0.1:37794, datanodeUuid=61b799ec-4cbf-4a0c-8b8d-445c84972a85, infoPort=37473, infoSecurePort=0, ipcPort=39000, storageInfo=lv=-57;cid=testClusterID;nsid=1665949480;c=1542830168067), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2018-11-21 14:56:11,703 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Successfully sent block report 0x794ad06166323063,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-11-21 14:56:11,703 [BP-117401205-192.168.200.57-1542830168067 heartbeating to localhost/127.0.0.1:38997] INFO  org.apache.hadoop.hdfs.server.datanode.DataNode  - Got finalize command for block pool BP-117401205-192.168.200.57-1542830168067
2018-11-21 14:56:11,742 [main] INFO  org.apache.hadoop.hdfs.MiniDFSCluster  - Cluster is active
2018-11-21 14:56:11,774 [IPC Server handler 9 on 38997] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=enquery (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=\/user\/enquery	dst=null	perm=enquery:supergroup:rwxr-xr-x	proto=rpc
2018-11-21 14:56:11,857 [IPC Server handler 6 on 38997] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=enquery (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=\/tmp\/hadoop-yarn\/staging	dst=null	perm=null	proto=rpc
2018-11-21 14:56:11,858 [main] INFO  org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster  - mkdir: hdfs://localhost:38997/tmp/hadoop-yarn/staging
2018-11-21 14:56:11,860 [IPC Server handler 8 on 38997] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=enquery (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=\/tmp\/hadoop-yarn\/staging	dst=null	perm=enquery:supergroup:rwxrwxrwx	proto=rpc
2018-11-21 14:56:11,877 [main] INFO  org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils  - Default file system is set solely by core-default.xml therefore -  ignoring
2018-11-21 14:56:11,881 [IPC Server handler 0 on 38997] INFO  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit  - allowed=true	ugi=enquery (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=\/tmp\/hadoop-yarn\/staging\/history\/done	dst=null	perm=enquery:supergroup:rwxrwxrwx	proto=rpc
2018-11-21 14:56:11,886 [main] INFO  org.apache.hadoop.net.ServerSocketUtil  - Using port 9188
2018-11-21 14:56:11,894 [main] INFO  org.apache.hadoop.conf.Configuration  - resource-types.xml not found
2018-11-21 14:56:11,895 [main] INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils  - Unable to find 'resource-types.xml'.
2018-11-21 14:56:11,942 [main] INFO  org.apache.hadoop.conf.Configuration  - core-site.xml not found
2018-11-21 14:56:11,942 [main] INFO  org.apache.hadoop.security.Groups  - clearing userToGroupsMap cache
2018-11-21 14:56:11,965 [main] INFO  org.apache.hadoop.conf.Configuration  - yarn-site.xml not found
2018-11-21 14:56:11,967 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2018-11-21 14:56:11,974 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM  - NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2018-11-21 14:56:11,976 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager  - ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2018-11-21 14:56:11,979 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager  - AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2018-11-21 14:56:11,998 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2018-11-21 14:56:12,000 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2018-11-21 14:56:12,001 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.ResourceManager  - Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2018-11-21 14:56:12,022 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
2018-11-21 14:56:12,023 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2018-11-21 14:56:12,023 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2018-11-21 14:56:12,024 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2018-11-21 14:56:12,028 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - ResourceManager metrics system started (again)
2018-11-21 14:56:12,043 [main] INFO  org.apache.hadoop.yarn.security.YarnAuthorizationProvider  - org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
2018-11-21 14:56:12,050 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2018-11-21 14:56:12,065 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2018-11-21 14:56:12,068 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo  - Registered RMNMInfo MBean
2018-11-21 14:56:12,068 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor  - Application lifelime monitor interval set to 3000 ms.
2018-11-21 14:56:12,077 [main] INFO  org.apache.hadoop.util.HostsFileReader  - Refreshing hosts (include/exclude) list
2018-11-21 14:56:12,084 [main] INFO  org.apache.hadoop.conf.Configuration  - found resource capacity-scheduler.xml at jar:file:/home/enquery/.m2/repository/org/apache/hadoop/hadoop-yarn-server-tests/3.0.1/hadoop-yarn-server-tests-3.0.1-tests.jar!/capacity-scheduler.xml
2018-11-21 14:56:12,136 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler  - Minimum allocation = <memory:1024, vCores:1>
2018-11-21 14:56:12,136 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler  - Maximum allocation = <memory:8192, vCores:4>
2018-11-21 14:56:12,166 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration  - max alloc mb per queue for root is undefined
2018-11-21 14:56:12,166 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration  - max alloc vcore per queue for root is undefined
2018-11-21 14:56:12,171 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue  - root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
2018-11-21 14:56:12,171 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue  - Initialized parent-queue root name=root, fullname=root
2018-11-21 14:56:12,186 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration  - max alloc mb per queue for root.default is undefined
2018-11-21 14:56:12,186 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration  - max alloc vcore per queue for root.default is undefined
2018-11-21 14:56:12,189 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue  - Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
nodeLocalityDelay = -1
rackLocalityAdditionalDelay = -1
labels=*,
reservationsContinueLooking = true
preemptionDisabled = true
defaultAppPriorityPerQueue = 0
priority = 0
maxLifetime = -1 seconds
defaultLifetime = -1 seconds
2018-11-21 14:56:12,189 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager  - Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0
2018-11-21 14:56:12,189 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager  - Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2018-11-21 14:56:12,190 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager  - Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2018-11-21 14:56:12,190 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler  - Initialized queue mappings, override: false
2018-11-21 14:56:12,191 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler  - Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2018-11-21 14:56:12,194 [main] INFO  org.apache.hadoop.conf.Configuration  - dynamic-resources.xml not found
2018-11-21 14:56:12,197 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain  - Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
2018-11-21 14:56:12,200 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.ResourceManager  - TimelineServicePublisher is not configured
2018-11-21 14:56:12,202 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.MiniYARNCluster$1
2018-11-21 14:56:12,203 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Created localDir in /home/enquery/staging/EncryptedQuery/pig/target/PigMiniCluster/PigMiniCluster-localDir-nm-0_0
2018-11-21 14:56:12,203 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Created localDir in /home/enquery/staging/EncryptedQuery/pig/target/PigMiniCluster/PigMiniCluster-localDir-nm-0_1
2018-11-21 14:56:12,203 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Created localDir in /home/enquery/staging/EncryptedQuery/pig/target/PigMiniCluster/PigMiniCluster-localDir-nm-0_2
2018-11-21 14:56:12,203 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Created localDir in /home/enquery/staging/EncryptedQuery/pig/target/PigMiniCluster/PigMiniCluster-localDir-nm-0_3
2018-11-21 14:56:12,203 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Created logDir in /home/enquery/staging/EncryptedQuery/pig/target/PigMiniCluster/PigMiniCluster-logDir-nm-0_0
2018-11-21 14:56:12,204 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Created logDir in /home/enquery/staging/EncryptedQuery/pig/target/PigMiniCluster/PigMiniCluster-logDir-nm-0_1
2018-11-21 14:56:12,204 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Created logDir in /home/enquery/staging/EncryptedQuery/pig/target/PigMiniCluster/PigMiniCluster-logDir-nm-0_2
2018-11-21 14:56:12,204 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Created logDir in /home/enquery/staging/EncryptedQuery/pig/target/PigMiniCluster/PigMiniCluster-logDir-nm-0_3
2018-11-21 14:56:12,205 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - Starting NM: 0
2018-11-21 14:56:12,221 [main] INFO  org.apache.hadoop.yarn.server.nodemanager.NodeManager  - Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.
2018-11-21 14:56:12,255 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher
2018-11-21 14:56:12,255 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher
2018-11-21 14:56:12,256 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper
2018-11-21 14:56:12,256 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices
2018-11-21 14:56:12,256 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl
2018-11-21 14:56:12,257 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher
2018-11-21 14:56:12,257 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler
2018-11-21 14:56:12,262 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.MiniYARNCluster$CustomContainerManagerImpl
2018-11-21 14:56:12,262 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.MiniYARNCluster$ShortCircuitedNodeManager
2018-11-21 14:56:12,262 [main] INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - NodeManager metrics system started (again)
2018-11-21 14:56:12,272 [main] INFO  org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection  - Disk Validator: yarn.nodemanager.disk-validator is loaded.
2018-11-21 14:56:12,280 [main] INFO  org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection  - Disk Validator: yarn.nodemanager.disk-validator is loaded.
2018-11-21 14:56:12,717 [main] INFO  org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl  -  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@6dcc40f5
2018-11-21 14:56:12,720 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler
2018-11-21 14:56:12,721 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService
2018-11-21 14:56:12,722 [main] INFO  org.apache.hadoop.yarn.server.MiniYARNCluster  - CustomAMRMProxyService is disabled
2018-11-21 14:56:12,722 [main] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService  - per directory file limit = 8192
2018-11-21 14:56:13,139 [main] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService  - Disk Validator: yarn.nodemanager.disk-validator is loaded.
2018-11-21 14:56:13,144 [main] INFO  org.apache.hadoop.yarn.event.AsyncDispatcher  - Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker
2018-11-21 14:56:13,173 [main] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices  - Adding auxiliary service mapreduce_shuffle, "mapreduce_shuffle"
2018-11-21 14:56:13,229 [main] INFO  org.apache.hadoop.service.AbstractService  - Service mapreduce_shuffle failed in state INITED; cause: org.jboss.netty.channel.ChannelException: Failed to create a selector.
org.jboss.netty.channel.ChannelException: Failed to create a selector.
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:362)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:100)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:52)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:44)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:80)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.apache.hadoop.mapred.ShuffleHandler.serviceInit(ShuffleHandler.java:520)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:167)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:315)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:447)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper.serviceInit(MiniYARNCluster.java:597)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:315)
	at org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster.serviceInit(MiniMRYarnCluster.java:205)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:88)
	at org.enquery.encryptedquery.pig.mini.MiniGenericCluster.buildCluster(MiniGenericCluster.java:80)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.buildCluster(MiniCluster.java:53)
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.io.IOException: Too many open files
	at sun.nio.ch.IOUtil.makePipe(Native Method)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:65)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at java.nio.channels.Selector.open(Selector.java:227)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.open(SelectorUtil.java:63)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:360)
	... 54 more
2018-11-21 14:56:13,237 [main] ERROR org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices  - Failed to initialize mapreduce_shuffle
org.jboss.netty.channel.ChannelException: Failed to create a selector.
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:362)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:100)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:52)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:44)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:80)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.apache.hadoop.mapred.ShuffleHandler.serviceInit(ShuffleHandler.java:520)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:167)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:315)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:447)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper.serviceInit(MiniYARNCluster.java:597)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:315)
	at org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster.serviceInit(MiniMRYarnCluster.java:205)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:88)
	at org.enquery.encryptedquery.pig.mini.MiniGenericCluster.buildCluster(MiniGenericCluster.java:80)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.buildCluster(MiniCluster.java:53)
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.io.IOException: Too many open files
	at sun.nio.ch.IOUtil.makePipe(Native Method)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:65)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at java.nio.channels.Selector.open(Selector.java:227)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.open(SelectorUtil.java:63)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:360)
	... 54 more
2018-11-21 14:56:13,237 [main] INFO  org.apache.hadoop.service.AbstractService  - Service org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices failed in state INITED; cause: org.jboss.netty.channel.ChannelException: Failed to create a selector.
org.jboss.netty.channel.ChannelException: Failed to create a selector.
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:362)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:100)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:52)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:44)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:80)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.apache.hadoop.mapred.ShuffleHandler.serviceInit(ShuffleHandler.java:520)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:167)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:315)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:447)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper.serviceInit(MiniYARNCluster.java:597)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:315)
	at org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster.serviceInit(MiniMRYarnCluster.java:205)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:88)
	at org.enquery.encryptedquery.pig.mini.MiniGenericCluster.buildCluster(MiniGenericCluster.java:80)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.buildCluster(MiniCluster.java:53)
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.io.IOException: Too many open files
	at sun.nio.ch.IOUtil.makePipe(Native Method)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:65)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at java.nio.channels.Selector.open(Selector.java:227)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.open(SelectorUtil.java:63)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:360)
	... 54 more
2018-11-21 14:56:13,238 [main] INFO  org.apache.hadoop.service.AbstractService  - Service org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl failed in state INITED; cause: org.jboss.netty.channel.ChannelException: Failed to create a selector.
org.jboss.netty.channel.ChannelException: Failed to create a selector.
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:362)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:100)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:52)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:44)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:80)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.apache.hadoop.mapred.ShuffleHandler.serviceInit(ShuffleHandler.java:520)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:167)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:315)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:447)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper.serviceInit(MiniYARNCluster.java:597)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:315)
	at org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster.serviceInit(MiniMRYarnCluster.java:205)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:88)
	at org.enquery.encryptedquery.pig.mini.MiniGenericCluster.buildCluster(MiniGenericCluster.java:80)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.buildCluster(MiniCluster.java:53)
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.io.IOException: Too many open files
	at sun.nio.ch.IOUtil.makePipe(Native Method)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:65)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at java.nio.channels.Selector.open(Selector.java:227)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.open(SelectorUtil.java:63)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:360)
	... 54 more
2018-11-21 14:56:13,239 [main] INFO  org.apache.hadoop.service.AbstractService  - Service NodeManager failed in state INITED; cause: org.jboss.netty.channel.ChannelException: Failed to create a selector.
org.jboss.netty.channel.ChannelException: Failed to create a selector.
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:362)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:100)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:52)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:44)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:80)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.apache.hadoop.mapred.ShuffleHandler.serviceInit(ShuffleHandler.java:520)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:167)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:315)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:447)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper.serviceInit(MiniYARNCluster.java:597)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:315)
	at org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster.serviceInit(MiniMRYarnCluster.java:205)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:88)
	at org.enquery.encryptedquery.pig.mini.MiniGenericCluster.buildCluster(MiniGenericCluster.java:80)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.buildCluster(MiniCluster.java:53)
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.io.IOException: Too many open files
	at sun.nio.ch.IOUtil.makePipe(Native Method)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:65)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at java.nio.channels.Selector.open(Selector.java:227)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.open(SelectorUtil.java:63)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:360)
	... 54 more
2018-11-21 14:56:13,239 [main] INFO  org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl  - Node Resource monitoring interval is <=0. org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is disabled.
2018-11-21 14:56:13,239 [main] INFO  org.apache.hadoop.service.AbstractService  - Service org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper_0 failed in state INITED; cause: org.jboss.netty.channel.ChannelException: Failed to create a selector.
org.jboss.netty.channel.ChannelException: Failed to create a selector.
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:362)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:100)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:52)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:44)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:80)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.apache.hadoop.mapred.ShuffleHandler.serviceInit(ShuffleHandler.java:520)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:167)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:315)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:447)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper.serviceInit(MiniYARNCluster.java:597)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:315)
	at org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster.serviceInit(MiniMRYarnCluster.java:205)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:88)
	at org.enquery.encryptedquery.pig.mini.MiniGenericCluster.buildCluster(MiniGenericCluster.java:80)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.buildCluster(MiniCluster.java:53)
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.io.IOException: Too many open files
	at sun.nio.ch.IOUtil.makePipe(Native Method)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:65)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at java.nio.channels.Selector.open(Selector.java:227)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.open(SelectorUtil.java:63)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:360)
	... 54 more
2018-11-21 14:56:13,240 [main] INFO  org.apache.hadoop.service.AbstractService  - Service PigMiniCluster failed in state INITED; cause: org.jboss.netty.channel.ChannelException: Failed to create a selector.
org.jboss.netty.channel.ChannelException: Failed to create a selector.
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:362)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:100)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:52)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:44)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.newWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:80)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.apache.hadoop.mapred.ShuffleHandler.serviceInit(ShuffleHandler.java:520)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:167)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:315)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:447)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$NodeManagerWrapper.serviceInit(MiniYARNCluster.java:597)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.serviceInit(MiniYARNCluster.java:315)
	at org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster.serviceInit(MiniMRYarnCluster.java:205)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:88)
	at org.enquery.encryptedquery.pig.mini.MiniGenericCluster.buildCluster(MiniGenericCluster.java:80)
	at org.enquery.encryptedquery.pig.mini.MiniCluster.buildCluster(MiniCluster.java:53)
	at org.enquery.encryptedquery.pig.udfs.RowHashUdfTest.<clinit>(RowHashUdfTest.java:41)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.io.IOException: Too many open files
	at sun.nio.ch.IOUtil.makePipe(Native Method)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:65)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at java.nio.channels.Selector.open(Selector.java:227)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.open(SelectorUtil.java:63)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:360)
	... 54 more
2018-11-21 14:56:13,240 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.ResourceManager  - Transitioning to standby state
2018-11-21 14:56:13,240 [main] INFO  org.apache.hadoop.yarn.server.resourcemanager.ResourceManager  - Transitioned to standby state
]]></system-out>
  </testcase>
  <testcase name="org.enquery.encryptedquery.pig.udfs.RowHashUdfTest" classname="org.enquery.encryptedquery.pig.udfs.RowHashUdfTest" time="0.001">
    <error message="Could not initialize class org.enquery.encryptedquery.pig.udfs.RowHashUdfTest" type="java.lang.NoClassDefFoundError">java.lang.NoClassDefFoundError: Could not initialize class org.enquery.encryptedquery.pig.udfs.RowHashUdfTest
</error>
  </testcase>
</testsuite>